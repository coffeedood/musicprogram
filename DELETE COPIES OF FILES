import os
import hashlib
from collections import defaultdict

def get_file_hash(filename, block_size=65536):
    sha1 = hashlib.sha1()
    with open(filename, 'rb') as f:
        while True:
            data = f.read(block_size)
            if not data:
                break
            sha1.update(data)
    return sha1.hexdigest()

def find_and_remove_duplicates(source_dir, target_dir, file_extension):
    source_file_size_dict = defaultdict(list)

    # Build a dictionary of source file sizes and corresponding paths
    for foldername, subfolders, filenames in os.walk(source_dir):
        for filename in filenames:
            if filename.endswith(file_extension):
                file_path = os.path.join(foldername, filename)
                file_size = os.path.getsize(file_path)
                source_file_size_dict[(filename, file_size)].append(file_path)

    # Iterate through the target directory and check for duplicates
    for foldername, subfolders, filenames in os.walk(target_dir):
        for filename in filenames:
            if filename.endswith(file_extension):
                file_path = os.path.join(foldername, filename)
                file_size = os.path.getsize(file_path)
                source_paths = source_file_size_dict.get((filename, file_size), [])

                # If there are duplicates in the source directory, remove the file from the target directory
                if len(source_paths) > 0:
                    print(f"Duplicate file found in target directory '{file_path}'; removing...")
                    os.remove(file_path)

if __name__ == "__main__":
    source_directory = "/media/riley/New Volume1"
    target_directory = "/media/riley/DRIVE"
    file_extension = ".mkv"
    
    find_and_remove_duplicates(source_directory, target_directory, file_extension)
